## 1. Сбор и представление данных

- **Набор данных** — это просто списки чисел. Каждое число в списке — это характеристика (признак) звукового фрагмента голоса (например, сила голоса, высота тона, тембр и т. д.), заранее вырезанного и оцифрованного микрофоном.  
- Есть две группы: «Директор» и «Бухгалтер». По три примера каждого мы подали в программу в виде массивов из 15 чисел.

---

## 2. Исследование данных (`explore_data`)

Эта функция просто выводит:

1. **Сколько** есть примеров для каждой роли (должно быть по три).  
2. **Средние значения** и **разброс** (стандартное отклонение) по каждому из 15 признаков.  
   
Чтобы приглянуться к «типичному» голосу директора и бухгалтера: где «центр тяжести» данных, а где они могут немного «разбегаться».

---

## 3. Подготовка и обучение модели (`build_model`)

1. **Масштабирование** (`StandardScaler`):  
   Приводит все признаки к единому масштабу (каждый признак после этого имеет среднее 0 и разброс 1). Это важно, чтобы признаки с большими и маленькими числами не «портят» обучение.

2. **Логистическая регрессия** (`LogisticRegression`):  
   Несмотря на слово «регрессия», в этой задаче она работает как **классификатор**: по вектору из 15 чисел пытается угадать, директор это или бухгалтер.  
   - Модель «учится» так: она подставляет разные веса для каждого из 15 признаков, чтобы максимально чётко разделить примеры директора и бухгалтера.

3. **Центроиды и границы**:  
   - Для каждого класса (директор/бухгалтер) считается **центроид** — усреднённый вектор всех его примеров (своего рода «идеальный голос»).  
   - Вычисляется **максимальное расстояние** от каждой точки класса до этого центроида. Это показывает, насколько разномастны голоса внутри группы.

Давайте разберём функцию `build_model` по шагам, с привязкой к формулам и математическим определениям.


### Подготовка данных

```python
X = np.vstack([s for samples in data.values() for s in samples])
y = np.hstack([[label] * len(samples) for label, samples in data.items()])
```

- Мы представляем все голосовые образцы в виде **матрицы признаков**  
  \[
    X = 
    \begin{pmatrix}
      x_{1,1} & x_{1,2} & \dots & x_{1,d} \\
      x_{2,1} & x_{2,2} & \dots & x_{2,d} \\
      \vdots  & \vdots  & \ddots & \vdots  \\
      x_{N,1} & x_{N,2} & \dots & x_{N,d}
    \end{pmatrix}
  \]  
  где \(N\) — общее число образцов (здесь 6), а \(d=15\) — число признаков в каждом.

- Вектор меток  
  \[
    y = (\,y_1, y_2, \dots, y_N\,)
  \]  
  в котором \(y_i\in\{\text{«Директор»},\text{«Бухгалтер»}\}\).

### Масштабирование признаков (StandardScaler)

```python
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

Для каждой колонки-признака \(j\) мы вычисляем

1. среднее  
   \[
     \mu_j = \frac{1}{N}\sum_{i=1}^N x_{i,j}
   \]
2. стандартное отклонение  
   \[
     \sigma_j = \sqrt{\frac{1}{N}\sum_{i=1}^N (x_{i,j} - \mu_j)^2}
   \]

Затем переводим каждый признак в «стандартизованный» вид:

\[
  \tilde x_{i,j} = \frac{x_{i,j} - \mu_j}{\sigma_j}
\quad\Longrightarrow\quad
X_{\text{scaled}} = [\tilde x_{i,j}].
\]

Это нужно, чтобы все признаки имели одинаковый вес при обучении модели.

### Логистическая регрессия

```python
clf = LogisticRegression(multi_class="multinomial", solver="lbfgs")
clf.fit(X_scaled, y)
```

#### Суть метода

Для многоклассовой логистической регрессии у нас есть весовые векторы \(w_k\in\mathbb{R}^d\) для каждого класса \(k\) (здесь «Директор» и «Бухгалтер»). Вероятность предсказания класса \(k\) на векторе \(\tilde x_i\) задаётся софтмаксом:

\[
  P(y_i = k \mid \tilde x_i)
  = \frac{\exp(w_k^\mathsf{T}\,\tilde x_i)}{\sum_{m}\exp(w_m^\mathsf{T}\,\tilde x_i)}.
\]

Алгоритм обучения ищет такие \(w_k\), которые максимизируют правдоподобие обучающей выборки, или эквивалентно минимизируют кросс-энтропийный (лог-лихтостный) риск:

\[
  \min_{w}\;
  -\sum_{i=1}^N \log P(y_i \mid \tilde x_i).
\]

Оптимизация происходит методом **LBFGS** (подходит для многомерной, многоклассовой задачи).

### Вычисление центроидов и максимальных расстояний

```python
centroids = {}
max_dists = {}
for label in data.keys():
    class_samples = X_scaled[y == label]
    centroid = class_samples.mean(axis=0)
    centroids[label] = centroid

    dists = np.linalg.norm(class_samples - centroid, axis=1)
    max_dists[label] = dists.max()
```

#### Центроид

Для каждого класса \(k\) (например, «Директор») берём все его стандартизованные векторы  
\(\{\tilde x_i : y_i = k\}\) и считаем их среднее:

\[
  c_k = \frac{1}{n_k}\sum_{\,i:y_i=k}\tilde x_i,
\]
где \(n_k\) — число образцов класса \(k\).  
Это точка «центр тяжести» голосов класса.

#### Расстояния до центроида

Для каждого обучающего вектора \(\tilde x_i\) вычисляем его евклидово расстояние до центроида:

\[
  d_i = \|\tilde x_i - c_k\|_2
  = \sqrt{\sum_{j=1}^d (\tilde x_{i,j} - c_{k,j})^2}\,.
\]

И сохраняем  
\(\max d_i = \max_{i:y_i=k} \|\tilde x_i - c_k\|\)  
как меру «разброса» голосов внутри класса.

### Итоговый результат функции

```python
return scaler, clf, centroids, max_dists
```

- **scaler** — объект, который умеет переводить новые векторы в стандартизованный вид.
- **clf** — обученная логистическая регрессия, которая даёт P(class|\(x\)).
- **centroids** — словарь \(\{k: c_k\}\) для каждого класса.
- **max_dists** — словарь \(\{k: \max_i d_i\}\).

Эти четыре компонента потом используются в функции `simulate_access`, чтобы проверить **и** уверенность модели (вероятность), **и** насколько новый голос «вписывается» в обученный кластер (расстояние до \(c_k\)). Если одно из условий нарушено — доступ отклоняется.

---

## 4. Решение о доступе (`simulate_access`)

Для каждого нового образца голоса (может быть как один из штатных, так и «чужой») мы делаем два главных шага:

1. **Оцениваем вероятность** (через ту же логистическую регрессию) того, что это директор или бухгалтер.  
2. **Считаем расстояние** от нового образца до «идеального голоса» того класса, в который модель его «смотрит».

**Условие, чтобы открыть замок**:  
- Модель уверена в своём выборе (вероятность ≥ 60 %).  
- И голос находится достаточно близко к центроиду (расстояние не больше чем 1.2×максимального «разбега» на обучении).

Если хотя бы одно из этих условий не выполняется, это считается **неизвестным** — и доступ **отклоняется**.
